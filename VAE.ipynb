{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bb5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafcf282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE1D(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE1D主要函数\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_length=256, latent_dim=32, base_channels=16):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.base_channels = base_channels\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, base_channels, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv1d(base_channels, base_channels*2, 4, 2, 1), nn.ReLU(),\n",
    "            nn.Conv1d(base_channels*2, base_channels*4, 4, 2, 1), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 256 → 128 → 64 → 32\n",
    "        self.reduced_len = input_length // 8\n",
    "        flat_dim = base_channels * 4 * self.reduced_len\n",
    "\n",
    "        # \n",
    "        self.fc_mu = nn.Linear(flat_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flat_dim, latent_dim)\n",
    "        self.fc_z = nn.Linear(latent_dim, flat_dim)\n",
    "\n",
    "        # 解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_channels*4, base_channels*2, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(base_channels*2, base_channels, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(base_channels, 1, 4, 2, 1) # 删掉激活函数\n",
    "        )\n",
    "\n",
    "    # 重参数化\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "#-----------------新加的-----------------------\n",
    "    def select_latent_dims(self, z, selected_dims):\n",
    "        \"\"\"\n",
    "        选择特定的潜变量维度\n",
    "        \n",
    "        参数:\n",
    "        - z: 完整的潜变量 [batch_size, latent_dim]\n",
    "        - selected_dims: 要选择的维度索引列表\n",
    "        \"\"\"\n",
    "        mask = torch.zeros_like(z)\n",
    "        mask[:, selected_dims] = 1\n",
    "        return z * mask\n",
    "    \n",
    "    def decode_from_selected_dims(self, z_selected):\n",
    "        \"\"\"\n",
    "        从选择的潜变量维度进行解码\n",
    "        \"\"\"\n",
    "        batch_size = z_selected.size(0)\n",
    "        x = self.fc_z(z_selected)\n",
    "        reduced_len = x.numel() // (batch_size * self.base_channels * 4)\n",
    "        x = x.view(batch_size, self.base_channels * 4, reduced_len)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x, selected_dims=None):\n",
    "        x = self.encoder(x)\n",
    "        batch_size, _, _ = x.shape\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # 使用选择的维度\n",
    "        if selected_dims is not None:\n",
    "            z = self.select_latent_dims(z, selected_dims)\n",
    "        \n",
    "        x_recon = self.decode_from_selected_dims(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "    \n",
    "    \n",
    "def analyze_latent_importance(model, dataloader, device, num_batches=10):\n",
    "    \"\"\"\n",
    "    分析每个潜变量维度的重要性\n",
    "    \n",
    "    参数:\n",
    "    - model: 训练好的VAE模型\n",
    "    - dataloader: 数据加载器\n",
    "    - device: 设备 (CPU/GPU)\n",
    "    - num_batches: 用于分析的批次数量\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    importance_scores = torch.zeros(model.latent_dim).to(device)\n",
    "    batch_count = 0\n",
    "    \n",
    "    print(\"开始分析潜变量维度重要性...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, _, _) in enumerate(dataloader):\n",
    "            if batch_idx >= num_batches:  # 只使用前几个批次以节省时间\n",
    "                break\n",
    "                \n",
    "            x_batch = x_batch.to(device)  # 正常前向传播一次（得到完整重建，不用于评分）\n",
    "            recon_full, _, _, z_full = model(x_batch)\n",
    "            \n",
    "            # 计算每个维度单独的重建质量\n",
    "            for dim in tqdm(range(model.latent_dim), desc=f\"分析批次 {batch_idx+1}/{num_batches}\"):\n",
    "                # 只使用当前维度\n",
    "                recon_dim = model(x_batch, selected_dims=[dim])[0]\n",
    "                # 计算重建损失作为重要性指标（损失越小，维度越重要）\n",
    "                loss = F.mse_loss(recon_dim, x_batch, reduction='mean')\n",
    "                importance_scores[dim] += loss.item()\n",
    "            \n",
    "            batch_count += 1\n",
    "    \n",
    "    # 平均损失\n",
    "    importance_scores /= batch_count\n",
    "    \n",
    "    # 按重要性排序（损失越小越重要）\n",
    "    sorted_indices = torch.argsort(importance_scores)\n",
    "    \n",
    "    print(\"\\n潜变量维度重要性分析完成!\")\n",
    "    print(f\"最重要维度: {sorted_indices[:5].cpu().numpy()}\")\n",
    "    print(f\"最不重要维度: {sorted_indices[-5:].cpu().numpy()}\")\n",
    "    \n",
    "    return sorted_indices.cpu().numpy(), importance_scores.cpu().numpy()\n",
    "\n",
    "def select_and_visualize_dims(importance_scores, num_selected=20):\n",
    "    \"\"\"\n",
    "    选择最重要的维度并可视化\n",
    "    \n",
    "    参数:\n",
    "    - importance_scores: 每个维度的重要性分数\n",
    "    - num_selected: 要选择的维度数量\n",
    "    \"\"\"\n",
    "    # 按重要性排序（损失越小越重要）\n",
    "    sorted_indices = np.argsort(importance_scores)\n",
    "    \n",
    "    # 选择最重要的维度\n",
    "    selected_dims = sorted_indices[:num_selected].tolist()\n",
    "    \n",
    "    print(f\"选择了最重要的 {num_selected} 个维度:\")\n",
    "    print(f\"维度索引: {selected_dims}\")\n",
    "    \n",
    "    # 可视化重要性\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 绘制所有维度的重要性\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(importance_scores)), importance_scores)\n",
    "    plt.xlabel('latent_dim')\n",
    "    plt.ylabel('reconstruct loss')\n",
    "    plt.title('Importance of latent_dim')\n",
    "    \n",
    "    # 标记选择的维度\n",
    "    plt.scatter(selected_dims, importance_scores[selected_dims], \n",
    "               color='red', s=50, zorder=5, label='selected dim')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 绘制排序后的重要性\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sorted_scores = np.sort(importance_scores)\n",
    "    plt.plot(range(len(sorted_scores)), sorted_scores, 'b-', linewidth=2)\n",
    "    plt.xlabel('order of dim')\n",
    "    plt.ylabel('reconstruct loss')\n",
    "    plt.title('order of importance of latent_dim')\n",
    "    \n",
    "    # 标记选择的维度数量\n",
    "    plt.axvline(x=num_selected, color='red', linestyle='--', \n",
    "                label=f'select{num_selected}dim')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_dims\n",
    "\n",
    "def compare_reconstructions(model, dataloader, device, selected_dims, num_samples=3):\n",
    "    \"\"\"\n",
    "    比较完整重建和部分重建的效果\n",
    "    \n",
    "    参数:\n",
    "    - model: 训练好的VAE模型\n",
    "    - dataloader: 数据加载器\n",
    "    - device: 设备\n",
    "    - selected_dims: 选择的维度索引\n",
    "    - num_samples: 要显示的样本数量\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_batch, names, scales) in enumerate(dataloader):\n",
    "            if batch_idx >= 1:  # 只使用第一个批次\n",
    "                break\n",
    "                \n",
    "            x_batch = x_batch.to(device)\n",
    "            scales = scales.to(device)\n",
    "            \n",
    "            # 完整重建（使用全部维度）\n",
    "            recon_full, _, _, _ = model(x_batch)\n",
    "            \n",
    "            # 使用选择的维度重建\n",
    "            recon_selected, _, _, _ = model(x_batch, selected_dims=selected_dims)\n",
    "            \n",
    "            # 反归一化\n",
    "            x_original = x_batch * scales\n",
    "            recon_full_denorm = recon_full * scales\n",
    "            recon_selected_denorm = recon_selected * scales\n",
    "            \n",
    "            # 计算损失\n",
    "            loss_full = F.mse_loss(recon_full, x_batch).item()\n",
    "            loss_selected = F.mse_loss(recon_selected, x_batch).item()\n",
    "            \n",
    "            print(f\"完整重建损失: {loss_full:.4f}\")\n",
    "            print(f\"使用{len(selected_dims)}个维度的重建损失: {loss_selected:.4f}\")\n",
    "            print(f\"重建质量保持: {(1 - loss_selected/loss_full)*100:.2f}%\")\n",
    "            \n",
    "            # 在同一张图中比较原始谱线和重建谱线\n",
    "            for i in range(min(num_samples, x_batch.size(0))):\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # 获取数据\n",
    "                original_data = x_original[i].squeeze().cpu().numpy()\n",
    "                full_recon_data = recon_full_denorm[i].squeeze().cpu().numpy()\n",
    "                selected_recon_data = recon_selected_denorm[i].squeeze().cpu().numpy()\n",
    "                \n",
    "                # 绘制在同一张图上\n",
    "                plt.plot(original_data, 'k-', linewidth=2, label='original data')\n",
    "                plt.plot(full_recon_data, 'b-', linewidth=1.5, label='full reconstruct (32)', alpha=0.8)\n",
    "                plt.plot(selected_recon_data, 'r--', linewidth=1.5, label=f'select {len(selected_dims)} dim', alpha=0.8)\n",
    "                \n",
    "                plt.title(f'compare_reconstructions - {names[i]}')\n",
    "                plt.xlabel('fre')\n",
    "                plt.ylabel('flux')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # 添加文本说明\n",
    "                textstr = '\\n'.join((\n",
    "                    f'full reconstructions loss: {loss_full:.4f}',\n",
    "                    f'select{len(selected_dims)}dim to reconstruct: {loss_selected:.4f}',\n",
    "                    f'Maintaining the quality of reconstruction: {(1 - loss_selected/loss_full)*100:.2f}%'\n",
    "                ))\n",
    "                props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "                plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                        verticalalignment='top', bbox=props)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dad730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
